Prompt Engineering Tutorial for QA & GitHub Copilot

Book Structure (From Beginner to Expert)

1. Introduction to Prompt Engineering

Imagine you're speaking to a powerful assistant who knows almost everythingâ€”but only gives you the best response when you ask the right question. That "question" is your prompt.

Prompt Engineering is the art and science of crafting clear, goal-driven prompts to get the most accurate, useful responses from language models like ChatGPT and GitHub Copilot. In QA, it can automate tedious testing tasks, speed up debugging, and supercharge documentation.

ğŸ¯ Real-World Analogy:

Think of a prompt like a Jira ticket. If you write a vague one, the developer may ask 5 follow-ups. But if it's specific, it can go straight to execution.

ğŸ¤– In QA and automation, prompt engineering can help:

Generate robust Selenium or Rest Assured test cases

Discover edge cases that are easily missed

Explain or refactor existing test code

Summarize failed logs or compare versions of scripts

ğŸ§  Why It Matters:

GPT-4 can process 50+ pages of code at once

GitHub Copilot can suggest tests, functions, regex

You save hours on boilerplate and research

By the end of this book, you'll be able to:
âœ… Create powerful prompts for testing toolsâœ… Use GitHub Copilot like a smart pair programmerâœ… Build your own prompt librariesâœ… Teach others on your team how to do the same

2. Understanding Language Models (LLMs)

Before using them effectively, letâ€™s understand how LLMs work.

ğŸ§© Core Concepts

Token: The basic unit of input. "TestAutomation" = 2 tokens.

Temperature: Controls randomness. Lower = more accurate, Higher = more creative. For QA, stick to 0.2â€“0.5.

Context Window: LLM memory span. GPT-4 = ~32,000 tokens (~50 pages).

ğŸ’¡ LLM Capabilities for QA:

Generate structured code (tests, mocks, data factories)

Convert scenario steps into test functions

Explain complex functions in laymanâ€™s terms

Detect gaps in test coverage

â— Limitations:

May hallucinate functions or methods

Can't access real-time data or logs (unless you paste them)

Output depends entirely on your input clarity

âœ… Best Practice: Treat LLMs like junior engineers. If you give sloppy input, expect guesswork.

3. Anatomy of a Good Prompt

Letâ€™s say you want to generate a login test. You could say:âŒ "Write a login test." â€” too vague.âœ… "Write a Java Selenium test using Selenide to verify login with valid and invalid credentials, and assert expected error/success messages."

ğŸ› ï¸ 3 Parts of a Good Prompt

Instruction â€“ What to do ("Write a test case")

Context â€“ Language, framework, endpoint, data

Constraints â€“ Timeouts, response codes, structure

âœ¨ Prompt Formats

Zero-shot â€“ No examples. Just instructions.

Few-shot â€“ Include 1-2 examples first

Chain-of-thought â€“ Add logical reasoning steps

ğŸ” Example Progression

Zero-shot:

"Write a login test in Java."

Few-shot:

"Here are two test cases. Write one more for forgot password."

Chain-of-thought:

"Generate a test and explain how each step validates the feature."

âœ… Best Practice: Start simple, iterate. Always test your prompt.

4. Prompt Patterns and Templates

Use these templates daily in QA. Memorize or save them!

ğŸ§ª Code Generation

"Write a Java method to validate email format using regex."

ğŸ§ª Test Case Generation

"Generate a Selenium test using TestNG that opens the login page, enters credentials, clicks submit, and validates dashboard access."

ğŸ Bug Reproduction

"Simulate steps in Selenium to reproduce a 403 forbidden error when accessing /admin without login."

ğŸ§¹ Data Extraction

"Extract phone numbers from this JSON response."

ğŸ“Š Test Summary

"Summarize the difference between test_log_v1 and test_log_v2 in less than 5 bullet points."

âœ… Best Practice: Always include URLs, sample payloads, or schema in the prompt if needed.

5. Prompt Engineering for QA Automation

Letâ€™s walk through real-world QA prompting examples you can use today.

âœ… UI Test Case (Selenium/Selenide)

Basic Prompt:

"Write a Selenium Java test case using Selenide for login."

Better Prompt:

"Generate a Java UI test using Selenide that navigates to https://example.com/login, tests with valid and invalid credentials, checks error alerts, and verifies dashboard on successful login."

âœ… Do:

Specify site URL

Mention all user scenarios (valid/invalid)

Include success + failure conditions

ğŸš« Don't:

Ask generic prompts like "Make a UI test"

Forget assertions or output expectations

âœ… API Testing (Rest Assured)

Prompt:

"Generate a JUnit test using Rest Assured for POST /api/users with name and email. Expect 201. Add negative case for missing email."

Bonus Tip: Add sample JSON payload to your prompt for more precision.

âœ… Edge Case Discovery

Ask:

"List 7 edge cases for a registration form with name, email, phone."

Then follow-up:

"Convert to BDD format using Gherkin."

âœ… Scenario to Code Prompting

You write:

"Convert this test scenario into a Selenium TestNG test: 'User clicks forgot password, submits email, receives reset confirmation.'"

Ask for refinements:

"Now add assertion to verify toast message and navigation to homepage."

âœ… Best Practice: Treat prompt engineering like TDDâ€”iterate quickly.

6. Prompt Engineering with GitHub Copilot Enterprise

Letâ€™s now focus on Copilotâ€”your AI pair programmer inside IntelliJ or VSCode.

ğŸ’¡ Whatâ€™s Different in Copilot Enterprise?

Access to internal codebase context (with org permissions)

Custom-trained models based on enterprise workflows

Enhanced security and IP compliance

Markdown-aware, docstring-based completions

ğŸ–¥ï¸ Using Copilot in IntelliJ (QA Example)

Step-by-step for UI automation test prompt:

Open LoginTest.java

Type a comment:

// Test: Login with valid credentials using Selenide

Pause and watch Copilot generate a valid test function

Refine by adding inline prompts:

// Add negative case for blank password

Copilot understands context from filenames, previous code, and comment descriptions.

ğŸ› ï¸ Example Prompts Inside IDE:

// Test: POST /users returns 201 with valid payload

// Validate email field with incorrect format

// Extract phone number from user JSON response

âœ… Pro Tip: Copilot loves detailed comments. Donâ€™t just type â€œTest API.â€ Instead, describe the flow:

"Test POST /login with missing password. Assert 401 error response with error message."

âœ… Best Practices for Copilot Prompting:

Always begin with a clear comment describing what you want

Break down prompts: one action per line

Keep your function names descriptive (Copilot reads them!)

Provide JSON examples in multiline comments when needed

âš ï¸ Common Pitfalls:

ğŸš« Vague prompts: â€œDo login testâ€ğŸš« Missing assertions: Copilot may stop at UI interaction if no assertion is mentionedğŸš« Too many prompts in one comment block

ğŸ” Iteration Workflow:

Type high-level comment

Let Copilot autocomplete

Accept, review, and modify

Rerun and prompt again to refine

âœ… Pro Tip: Treat Copilot like a junior SDET. Review everything it writes.

7. Advanced Prompting Techniques

Once you're comfortable writing clear, structured prompts, it's time to level up. These advanced techniques let you do more with fewer prompts and get predictable, high-quality results.

ğŸ”— Prompt Chaining

Prompt chaining is the practice of breaking a big task into smaller logical steps and prompting each step one-by-one.

Example Use Case:

You want to generate test cases for a new signup feature.

First prompt:

"List 5 possible user journeys for signup on a typical e-commerce app."

Second prompt:

"For each journey, write a test case in Gherkin format."

Third prompt:

"Convert these Gherkin test cases into Java TestNG tests using Selenide."

âœ… Best Practice: Use this when the LLM fails to complete complex tasks in one go.

ğŸ§  System Prompts and Role-Based Prompting

Language models can behave differently based on the "persona" or role you assign them.

Example:

"You are a senior QA automation engineer. Write a Selenium test for the login feature with detailed comments and best practices."

This sets the tone for the response and influences quality.

âœ… Use role context when asking for structured, professional output or domain-specific tasks.

ğŸ“¦ JSON & Structured Output Prompts

For QA reporting, test data generation, or integrations, you often want structured responses like JSON.

Example:

"Generate 3 negative test cases for a login API. Output each case as JSON with keys: test_name, input, expected_output, status_code."

Sample Output:

[
  {
    "test_name": "Missing password",
    "input": {"username": "user1"},
    "expected_output": "Password is required",
    "status_code": 400
  }
]

âœ… Pro Tip: Always state: â€œRespond only in JSON. No explanations.â€ to avoid extra text.

ğŸ§ª Combining Tools (LLM + Copilot + CLI)

Prompt engineering becomes more powerful when you combine tools:

Use ChatGPT to generate edge cases

Use Copilot to turn them into code

Use CLI tools to validate or run them

Workflow:

ChatGPT â†’ "List 10 edge cases for password reset"

ChatGPT â†’ "Generate Java test cases"

Copy to IDE â†’ Copilot continues code with test coverage

âœ… Best Practice: Keep a clipboard file or PromptPlaybook.md in your repo with reusable prompt chains.

8. Debugging & Iterating Prompts

Even the best prompts sometimes fail. The model might give:

Partial output

Wrong format

Repeated or irrelevant content

Like code, prompts need to be debugged and refined. Hereâ€™s how.

ğŸ” Common Prompt Failure Modes

Vague prompt â†’ Ambiguous or generic output

Overloaded prompt â†’ Confusing or incomplete response

No formatting instructions â†’ Output mixes explanation with code

No examples provided â†’ LLM guesses instead of following pattern

ğŸ”„ Prompt Refinement Workflow

Think of this like testing a flaky script:

Start with a simple prompt

âœ… "Write a Selenium test for login page."

Add context

âœ… "Use Selenide in Java. Input valid and invalid credentials."

Specify structure or format

âœ… "Respond only with code."

Test output and improve

âŒ If assertions are missing, re-prompt:

"Add assertions for success and failure states."

âœ… Pro Tip: Save every good version of a prompt that worked. Reuse it later.

ğŸ§ª Prompt A/B Testing

Test two prompt versions for the same task and compare which gives better, cleaner, or more accurate results.

Version A:

"Write a test case for login."

Version B:

"Generate a Java TestNG test using Selenide to test login on https://app.testsite.com. Check both valid and invalid flows and assert UI messages."

ğŸ“Š Evaluate output:

Which test has better assertions?

Is the code reusable?

Is it production-grade?

âœ… Build your own "prompt test cases" for quality control.

ğŸ—‚ï¸ Creating & Maintaining a Prompt Library

Once you find prompts that work reliably, donâ€™t keep them in memoryâ€”document them.

ğŸ“ Sample Prompt Library Structure:

/PromptPlaybook.md
  - UI Prompts
  - API Prompts
  - Data Validation
  - Debug Logs
  - Code Review

âœ… Best Practice: Add comments, versions, and when to use which prompt.

âœ… Organize by framework:

Selenide

Rest Assured

Postman

Cucumber (Gherkin)

ğŸ”§ Tools for Prompt Iteration

ChatGPT history â†’ Reuse previous successful prompts

Notion / Obsidian / GitHub Wiki â†’ To store structured prompt libraries

Versioned Prompt Sheets (Excel or Markdown) â†’ Track refinements over time

âœ… Use "prompt history" just like commit history. Learn what worked and why.



9. Collaborative Prompting

Just like we share test scripts, libraries, and frameworks in a QA teamâ€”prompt engineering can and should be collaborative.

ğŸ¤ Why Collaborate on Prompts?

Prompts that work for one engineer will likely work for others

Encourages prompt reuse, not reinvention

Reduces prompt failures and accelerates test generation

Teaches junior engineers better practices

ğŸ‘¥ Prompt Reviews (Like Code Reviews)

Start treating prompts as first-class test assets.
Have teammates review prompt phrasing, clarity, constraints, and expected outputs.

âœ… Good Prompt Review Checklist:

Is the intent clear?

Is the scope well-defined?

Are test conditions/assertions explicit?

Does the prompt avoid vague phrasing like "check" or "verify"?

âœ… Pro Tip: Add reviewed prompts to your teamâ€™s internal QA handbook.

ğŸ”„ Version Control for Prompts

Just like you maintain Git repos, create a version-controlled prompt library.

Structure:

/prompts
  â”œâ”€â”€ ui-tests
  â”œâ”€â”€ api-tests
  â”œâ”€â”€ testdata-generation
  â””â”€â”€ utils

README.md  â†’ How to use, when to use, examples

Each prompt file can include:

Description

Input prompt

Expected response format

Edge case notes

Links to working test examples

âœ… Tip: Maintain a changelog for evolving prompt logic

ğŸ’¼ Prompts in QA Onboarding

When new testers join your team:

Let them study your PromptPlaybook

Assign them a "prompt A/B challenge"

Review their results in a learning session

This builds prompting muscle memory across the org.

ğŸ“£ Communication + Prompts = Power

Pair programming with LLMs works even better when you:

Co-write prompts live in meetings

Screen share with junior QAs to show your prompting thought process

Convert real bug reports into promptable scenarios

Example:

"Given this real production bug report, write 3 promptable test scenarios and generate code with Copilot."

âœ… Prompt reviews = knowledge sharing + QA mentorship

10. Ethics & Guardrails

Using AI tools like ChatGPT or Copilot in a QA environment introduces powerful capabilitiesâ€”but also new risks. Ethics and guardrails ensure that your prompts:

Don't leak sensitive data

Don't result in unsafe or biased outputs

Respect organizational compliance policies

Letâ€™s break it down by what to watch out for, and how to mitigate risk.

ğŸš« Common Prompting Risks in QA

1. Leaking Sensitive Info

Prompts like:

"Generate tests for our internal payment gateway code snippet"

...may expose business logic, secrets, or tokens if shared publicly or copied into an online LLM with no data protection.

âœ… Safe Practice: Use only enterprise-level tools like GitHub Copilot Enterprise or on-prem LLMs for sensitive repos.

2. Hallucinations (False Information)

LLMs may invent classes, methods, or API endpoints that donâ€™t exist.

Example:

"Write a Selenium test for UserController.validateToken()" (but no such method exists)

âœ… Always review generated code manually before committing.âœ… Use Copilot suggestions as drafts, not production code.

3. Bias or Discriminatory Suggestions

Even in QA, if you're testing forms with user data, poorly framed prompts may suggest:

Gendered test data only

Culturally biased names or assumptions

âœ… Provide neutral or inclusive context when generating data.âœ… Validate generated data fields for representation.

ğŸ›¡ï¸ Best Practices for Safe Prompting

âœ… Use AI in Secure Environments

Stick to Copilot Enterprise, which respects org boundaries

Disable prompts that include confidential user data

âœ… Keep Prompts Reproducible & Transparent

Save input/output pairs for audit

Share prompt versions with teammates

âœ… Train Your Team in Responsible Use

Include prompt ethics in onboarding

Review examples of dangerous or misleading prompts

âœ… Avoid Prompts Like:

â€œScan our entire repo and write all testsâ€  âŒ

â€œWrite tests for user auth with this private keyâ€  âŒ

âœ… Do This Instead:

â€œWrite a generic Selenide test for login, without using internal APIsâ€

â€œSuggest test coverage areas for this code snippet (no credentials included)â€

âœ… When in doubt, prompt with abstraction. Remove real tokens, emails, API secrets.

âš–ï¸ Legal & Compliance Notes

LLMs do not guarantee output is free of IP conflicts

Code copied from AI should always be reviewed & checked for license compatibility

Donâ€™t paste proprietary bug reports into free online tools

âœ… Use enterprise chat + enterprise Copilot + org GitHub account to stay safe.

11. Future of Prompt Engineering

Prompt engineering is not a static skillâ€”itâ€™s evolving fast. The future blends LLMs, automation agents, and retrieval systems into seamless workflows.

Letâ€™s look at the trends shaping the next era of QA + AI.

ğŸ¤– AI Agents for QA

AI agents are autonomous tools that:

Take goals as input ("achieve 90% test coverage")

Break it into subtasks

Prompt LLMs at each stage

Collect results and improve over time

ğŸ§  Example: Autonomous Test Generator Agent

Reads your API spec or Swagger doc

Generates test scenarios (prompt 1)

Converts to Gherkin syntax (prompt 2)

Generates Selenium code (prompt 3)

Logs test summary & coverage report (prompt 4)

âœ… Why it matters: You go from writing tests manually to defining what â€œcoverageâ€ means and letting the agent do the work.

ğŸ“š Retrieval-Augmented Generation (RAG)

RAG = combining LLMs with search/retrieval of internal documents.

ğŸ”§ QA Example:

â€œGenerate test cases for our signup API using our internal test strategy document + swagger.yamlâ€

LLM + RAG can:

Pull your orgâ€™s test templates

Reference actual business rules

Avoid hallucinating because it uses real source docs

âœ… Use RAG when you want outputs aligned to company standards
âœ… Platforms like OpenAIâ€™s GPTs or LangChain support RAG-based workflows

ğŸ” CI/CD + Prompt Automation

You can integrate prompts into your test pipelines!

Example Use Case:

In your GitHub Actions workflow:

On every new feature branch, trigger a prompt like:

"Suggest 3 regression test cases based on changes in this PR."

This can:

Speed up peer review

Suggest missed test areas

Auto-generate tests for small changes

âœ… Combine GitHub Copilot with GitHub Actions + ChatGPT API for magic

ğŸ”® Whatâ€™s Next?

LLM agents that file bugs, reproduce them, and suggest fixes

Prompt IDEs with autocomplete for prompt construction

Teams using prompt coverage metrics just like code coverage

â€œPromptOpsâ€ roles managing prompt workflows in CI/CD

âœ… Get ahead: start documenting prompts like test cases
âœ… Reuse + refine them as first-class QA assets

Congratulations! Youâ€™ve reached the end of the expert-level tutorial.
Next steps:

Use this material in your team seminar

Build your own prompt libraries

Share successful prompt patterns within your QA org

Youâ€™re now equipped to lead the prompt engineering revolution inside your team ğŸ’¡

